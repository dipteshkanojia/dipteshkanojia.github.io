[{"authors":["admin"],"categories":null,"content":"I am a Lecturer at Surrey Institute for People-Centred AI. My research addresses problems that intersect the areas of Natural Language Processing (NLP) and Machine or Deep Learning (ML/DL). The focus of my research is Machine Translation; working with Prof. Sabine Braun, Prof. ‚Ä™Constantin OrƒÉsan, Dr. F√©lix do Carmo and other colleagues.\nOther than that, my interests also lie in the NLP sub-areas of Cognitive NLP, Distributional and Lexical Semantics, Multimodality/Multilingualism in NLP, Sentiment/Emotion Analysis; and Teaching NLP. I graduated with a Joint PhD from the IITB-Monash Research Academy ( IIT Bombay, India \u0026amp; Monash University, Australia ) in 2021; and was advised by Prof. Pushpak Bhattacharyya, Prof. Gholamreza Haffari, and Prof. Malhar Kulkarni. Before starting my PhD, I was a Research Engineer at the CFILT Lab, IIT Bombay, India, where my investigations led to publications in diverse sub-areas of NLP and AI.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://dipteshkanojia.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Lecturer at Surrey Institute for People-Centred AI. My research addresses problems that intersect the areas of Natural Language Processing (NLP) and Machine or Deep Learning (ML/DL). The focus of my research is Machine Translation; working with Prof. Sabine Braun, Prof. ‚Ä™Constantin OrƒÉsan, Dr. F√©lix do Carmo and other colleagues.\nOther than that, my interests also lie in the NLP sub-areas of Cognitive NLP, Distributional and Lexical Semantics, Multimodality/Multilingualism in NLP, Sentiment/Emotion Analysis; and Teaching NLP.","tags":null,"title":"Diptesh Kanojia","type":"authors"},{"authors":["Diptesh Kanojia"],"categories":null,"content":"","date":1646395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646395200,"objectID":"5eb8696c81f8293440a1460b90e0c6de","permalink":"https://dipteshkanojia.github.io/talk/nmttalk/","publishdate":"2022-03-04T12:00:00Z","relpermalink":"/talk/nmttalk/","section":"talk","summary":"Should translators understand the power of the technology behind Google Translate and other providers of machine translation? We think so, and that is why we prepared this webinar as the first step in helping translators make informed decisions when working with this technology. The volume and quality of machine translation produced today baffles anyone who has been working or wants to start working with translation. Neural machine translation (NMT) is a set of the most advanced machine learning technologies that has been yielding surprising volumes of high-quality machine translation output. NMT uses neural network-based models to learn probabilistic models of languages, in such a way that they can be used to estimate very likely good translations of new source sentences. One of the key benefits of these approaches is to simplify the process of training MT systems. Unlike previous statistical translation systems, which consisted of many small sub-components that were tuned separately, NMT attempts to build and train a single, large neural network which will then be able to read a source sentence and output a translation for it. In this webinar, we will explain the use of widely available programming libraries to create a customised translation engine or model using NMT. The webinar will consist of a step-by-step demonstration of the different stages of training a NMT model, with simple explanations of the terms that are used by researchers that train these systems. The webinar will include introductory answers to questions like: What is a neural network? What are word embeddings? What does training and learning mean? How many stages are there in an NMT training process? What methods are there to improve the quality of the MT output? Why is the translation stage called decoding? There will be time for the participants to ask questions, but they are not expected to perform any hands-on work. As a follow-up to the webinar, we are preparing a short course with hands-on exercises for participants to train their own initial models. ","tags":["talks","machine translation","neural machine translation","translators"],"title":"Introducing Neural Machine Translation (NMT) to Translators","type":"talk"},{"authors":["Diptesh Kanojia"],"categories":["python","notifications","telegram","hacks"],"content":"There are many ways using which developers get updates from Python. Some use the \u0026lsquo;knockknock\u0026rsquo; library, some use the \u0026lsquo;wandb\u0026rsquo; package to track the progress and generate report, and so on. However, I recently found this nifty way of sending myself updates when, let us say, my machine/deep learning model has finished training or when my single-core-utilizing-python-code has finished executing which was taking an awful amount of time to do multiple simple lookups across a very large knowledge base (\u0026ldquo;been there, phew!\u0026quot;).\nThis blog post tries to provide you with steps to setup your own Telegram bot which can send you messages via Python. Your message is something you can personalize given an error thrown or a successful model training, but it will be a simple message notifying you that the process execution has come to a halt and you should probably tell your machine what to do next. I mean we have not yet developed smart ML/DL training processes which take care of all the possible model training and hyperparameter tuning on its own, right? :P\nYes, I know! but what if you wanted to have a look at the results and output on your own, and then change the hyperparameters accordingly?\nMotivated enough to do this? Let us get on with it!\nRequirements üëâ A Telegram Account: If you don‚Äôt have the Telegram app installed just download it from the play store. After downloading create an account using your mobile number just like WhatsApp.\nüëâ python-telegram-bot library: We will need a library module called python-telegram-bot. This library provides a pure Python interface for the Telegram Bot API. It‚Äôs compatible with Python versions 3.6.8+. In addition to the pure API implementation, this library features a number of high-level classes to make the development of bots easy and straightforward. These classes are contained in the ‚Äútelegram.ext‚Äù submodule. For more information, you can check their official GitHub repo.\nIt is a very simple process but has many steps so bear with me please.\nStep 1: Setup your own Bot!  Contact @BotFather on Telegram- Try to find this user in Telegram via your Phone application (Android or iOS) or via the PC version / macOS Version of the app downloadable via the official Telegram website. Here is what @BotFather looks like (just in case!).    üëâ Start the conversation with BotFather by pressing the \u0026lsquo;Start\u0026rsquo; button, and you will see a command \u0026lsquo;/start\u0026rsquo; initiate the conversation.\n  üëâ Now, type in the command to create a new bot to yourself \u0026lsquo;/newbot\u0026rsquo; and send this message. But wait, it will ask you various questions as shown below and help you setup this bot for you. It will ask you for a name (as you can see I was going for a name with an emoji which it denied me!); and you can also choose the bot picture there itself by sending it a picture or you can change it later.\n  üëâ You can see that I have redacted my token ID from the picture, but please copy your token ID and keep it noted as you will need it for steps further.\nDISCLAIMER ‚Äî Please save the token ID of the bot securely. Anyone with your token ID can manipulate this bot. üëâ Start a üí¨ conversation with you bot but do not expect any replies yet. :) \u0026ndash; Find the bot user on Telegram by using the username you had selected for the both. For example, the screenshot shows that my bot is named @diptesh_bot. Just write any message to it, say \u0026ldquo;Hi!\u0026quot;.\nüëâ Go to the URL: https://api.telegram.org/bot\u0026lt;tokenID\u0026gt;/getUpdates where you need to replace \u0026lt;tokenID\u0026gt; with your own bot token ID identified from the step above.\nüëâ The page shown is the response from the Telegram API which lets you know what messages your bot has received. Do not worry, we don\u0026rsquo;t have to keep tabs on this page, we just need to üí¨ (chat) ID from here and we shall move on to the Python codebase you need to setup. Look at the screenshot below and find the üí¨ ID from your API response.\n  For example, you can see that my üí¨ ID is 1600389501, note your üí¨ ID down as it is going to be used along with the token ID in Python later.\nStep 2: Setup your Python codebase!  The Python Setup- You can choose to go through the tutorials available on üí° GeeksforGeeks or the one on üí° CodeMentor. They appear on top of the Google search for \u0026lsquo;create telegram bot for python\u0026rsquo;. These tutorial provide you with a more extensive use of the API by the use of extended classes present with the python-telegram-bot library, however, you can also choose to complete this blog post tutorial which provides you with a simpler version of the same.\nüëâ Ensuring the code environment (optional) \u0026ndash; Since some of us use conda environments and virtual environments to execute our code with specific libraries (and their versions! do not get me started about the versioning and pip!), it is a good idea to ensure that you install the telegram library in the right environment. So change the environment via your terminal and activate the one you are going to use this bot with.\nüëâ Install the library - Execute the following via Terminal or via your notebook üìö:\n\u0026ndash; For terminal\npip install python-telegram-bot \u0026ndash; For notebooks, IDEs\n!pip install python-telegram-bot üëâ Once this installation finishes, open your code and simple add this cell as show below, or add it at the end of your code execution. In the code snippet shown below, replace the REDACTED-TOKEN-ID-FROM-POST with your token id inside the quotes, and use your üí¨ ID you noted down earlier to replace 0000000000.\nimport telegram bot = telegram.Bot(token=\u0026quot;REDACTED-TOKEN-ID-FROM-POST\u0026quot;) ## token ID needs to be obtained as shown earlier. bot.sendMessage(chat_id=0000000000, text=\u0026quot;Training is done\u0026quot;) ## chat_id needs to be obtained via the Telegram API as shown later.   Step 3: Execute! Execute your code- To watch this bot in action, execute the code and at its successfull execution you shall see that you recieve the message \u0026ldquo;Training is done\u0026rdquo; via the bot. Now if you have Telegram installed on your phone, you will recieve this message as long as you have internet connection and the Telegram API server is not down (which will hopefully not be the case!). If you add this snippet at the end of each model tranining your bot will probably end up having one-sided conversation with you; something like this:    üëâ More on this later, for now, setup this and have fun training models continously while getting progress notifications.\nThis ends the whole discussion on how to start getting notifications when your process execution comes to a halt using a Telegram bot. However, let us say you have co-authors, and all of you can run the code with different hyperparameters. Could you use the same steps above to send this message to a group? Why not create a telegram group and add your bot to it? Why not allow the bot to send messages to the group via a different üí¨ ID? I will try this in a few days and create a new post.\n","date":1642504393,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642504393,"objectID":"497d4e96f116d20472e2bafe3deedeaa","permalink":"https://dipteshkanojia.github.io/post/telegram-bot/","publishdate":"2022-01-18T11:13:13Z","relpermalink":"/post/telegram-bot/","section":"post","summary":"There are many ways using which developers get updates from Python. Some use the \u0026lsquo;knockknock\u0026rsquo; library, some use the \u0026lsquo;wandb\u0026rsquo; package to track the progress and generate report, and so on. However, I recently found this nifty way of sending myself updates when, let us say, my machine/deep learning model has finished training or when my single-core-utilizing-python-code has finished executing which was taking an awful amount of time to do multiple simple lookups across a very large knowledge base (\u0026ldquo;been there, phew!","tags":["python","notifications","telegram","hacks"],"title":"Creating a Telegram Bot to Share Updates via Python","type":"post"},{"authors":["Prashant Sharma","Hadeel Saadany","Leonardo Zilio","Diptesh Kanojia","Constantin OrƒÉsan"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"f8b1ca39c8e96022252d11cf0a7fc0f2","permalink":"https://dipteshkanojia.github.io/publication/sdu-st-aaai-2022-acronym/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/sdu-st-aaai-2022-acronym/","section":"publication","summary":"Acronyms are abbreviated units of a phrase constructed by using initial components of the phrase in a text. Automatic extraction of acronyms from a text can help various Natural Language Processing tasks like machine translation, information retrieval, and text summarisation. This paper discusses an ensemble approach for the task of Acronym Extraction, which utilises two different methods to extract acronyms and their corresponding long forms. The first method utilises a multilingual contextual language model and fine-tunes the model to perform the task. The second method relies on a convolutional neural network architecture to extract acronyms and append them to the output of the previous method. We also augment the official training dataset with additional training samples extracted from several open-access journals to help improve the task performance. Our dataset analysis also highlights the noise within the current task dataset. Our approach achieves the following macro-F1 scores on test data released with the task: Danish (0.74), English-Legal (0.72), English-Scientific (0.73), French (0.63), Persian (0.57), Spanish (0.65), Vietnamese (0.65). We release our code and models publicly.","tags":["acronym extraction","shared task","data analysis","data augmentation","ensemble approach"],"title":"An Ensemble Approach to Acronym Extraction using Transformers","type":"publication"},{"authors":["Diptesh Kanojia"],"categories":null,"content":"","date":1639872000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639872000,"objectID":"b387e7b7a613898df0114abd0426a332","permalink":"https://dipteshkanojia.github.io/talk/msrintalk/","publishdate":"2021-12-19T00:00:00Z","relpermalink":"/talk/msrintalk/","section":"talk","summary":"Current Machine Translation (MT) systems achieve very good results on a growing variety of language pairs and datasets. However, they are known to produce fluent translation outputs that can contain important meaning errors, thus undermining their reliability in practice. Quality Estimation (QE) is the task of automatically assessing the performance of MT systems at test time. Thus, in order to be useful, QE systems should be able to detect such errors. However, this ability is yet to be tested in the current evaluation practices, where QE systems are assessed only in terms of their correlation with human judgements. In this talk, we will discuss how we attempted to bridge this gap by proposing a general methodology for the adversarial testing of QE for MT. In this discussion, we first see that despite a high correlation with human judgements achieved by the recent SOTA, certain types of meaning errors are still problematic for QE to detect. We also discuss how the ability of a given model to discriminate between meaning-preserving and meaning-altering perturbations is predictive of its overall performance, thus potentially allowing us to compare the QE systems without relying on manual quality annotation. We also see possible future applications of this work and discuss various directions we are investigating to improve the performance of QE systems.","tags":["talks","quality estimation","machine translation","adversarial evaluation","probing systems"],"title":"Quality Estimation for Machine Translation","type":"talk"},{"authors":["Mrinal Rawat","Diptesh Kanojia"],"categories":null,"content":"","date":1639094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639094400,"objectID":"bceef8e3fe8d292a43c6d0efd02bbe01","permalink":"https://dipteshkanojia.github.io/publication/icon-2021-fakenews-evidence/","publishdate":"2021-12-10T00:00:00Z","relpermalink":"/publication/icon-2021-fakenews-evidence/","section":"publication","summary":"Fake news, misinformation, and unverifiable facts on social media platforms propagate disharmony and affect society, especially when dealing with an epidemic like COVID-19. The task of Fake News Detection aims to tackle the effects of such misinformation by classifying news items as fake or real. In this paper, we propose a novel approach that improves over the current automatic fake news detection approaches by automatically gathering evidence for each claim. Our approach extracts supporting evidence from the web articles and then selects appropriate text to be treated as evidence sets. We use a pre-trained summarizer on these evidence sets and then use the extracted summary as supporting evidence to aid the classification task. Our experiments, using both machine learning and deep learning-based methods, help perform an extensive evaluation of our approach. The results show that our approach outperforms the state-of-the-art methods in fake news detection to achieve an F1-score of 99.25 over the dataset provided for the CONSTRAINT-2021 Shared Task. We also release the augmented dataset, our code and models for any further research.","tags":["fake news","fake news detection","evidence collection","evidence summarization"],"title":"Automated Evidence Collection for Fake News Detection","type":"publication"},{"authors":["Diptesh Kanojia","Marina Fomicheva","Tharindu Ranasinghe","Fr√©d√©ric Blain","Constantin OrƒÉsan","Lucia Specia"],"categories":null,"content":"","date":1632528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632528000,"objectID":"65a770bb3ef86536de758ef961998215","permalink":"https://dipteshkanojia.github.io/publication/wmt-2021-qeeval/","publishdate":"2021-09-25T00:00:00Z","relpermalink":"/publication/wmt-2021-qeeval/","section":"publication","summary":"Current Machine Translation (MT) systems achieve very good results on a growing variety of language pairs and datasets. However, they are known to produce fluent translation outputs that can contain important meaning errors, thus undermining their reliability in practice. Quality Estimation (QE) is the task of automatically assessing the performance of MT systems at test time. Thus, in order to be useful, QE systems should be able to detect such errors. However, this ability is yet to be tested in the current evaluation practices, where QE systems are assessed only in terms of their correlation with human judgements. In this work, we bridge this gap by proposing a general methodology for adversarial testing of QE for MT. First, we show that despite a high correlation with human judgements achieved by the recent SOTA, certain types of meaning errors are still problematic for QE to detect. Second, we show that on average, the ability of a given model to discriminate between meaning-preserving and meaning-altering perturbations is predictive of its overall performance, thus potentially allowing for comparing QE systems without relying on manual quality annotation. This paper is also available on [ACL Anthology](https://aclanthology.org/2021.wmt-1.67/).","tags":["quality estimation","evaluation","machine translation","perturbations","empirical"],"title":"Pushing the Right Buttons: Adversarial Evaluation of Quality Estimation","type":"publication"},{"authors":["Anirudh Mittal","Pranav Jeevan","Prerak Gandhi","Diptesh Kanojia","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1632441600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632441600,"objectID":"e0ec231d417796a9a90451e4093e8b45","permalink":"https://dipteshkanojia.github.io/publication/emnlp-2021-standup/","publishdate":"2021-09-24T00:00:00Z","relpermalink":"/publication/emnlp-2021-standup/","section":"publication","summary":"Computational Humour (CH) has attracted the interest of Natural Language Processing and Computational Linguistics communities. Creating datasets for automatic measurement of humour quotient is difficult due to multiple possible interpretations of the content. In this work, we create a multi-modal humour-annotated dataset (~40 hours) using stand-up comedy clips. We devise a novel scoring mechanism to annotate the training data with a humour quotient score using the audience's laughter. The normalized duration (laughter duration divided by the clip duration) of laughter in each clip is used to compute this humour coefficient score on a five-point scale (0-4). This method of scoring is validated by comparing with manually annotated scores, wherein a quadratic weighted kappa of 0.6 is obtained. We use this dataset to train a model that provides a 'funniness' score, on a five-point scale, given the audio and its corresponding text. We compare various neural language models for the task of humour-rating and achieve an accuracy of 0.813 in terms of Quadratic Weighted Kappa (QWK). Our 'Open Mic' dataset is released for further research along with the code.","tags":["computational humour","rating humour","standup comedy","multimodal","dataset","empirical"],"title":"'So You Think You‚Äôre Funny?': Rating the Humour Quotient in Standup Comedy","type":"publication"},{"authors":["Girishkumar Ponkiya","Diptesh Kanojia","Pushpak Bhattacharyya","Girish Palshikar"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"a2099c867e03f4ac2f0c8f970807cae3","permalink":"https://dipteshkanojia.github.io/publication/aclfind-2021-nci/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/publication/aclfind-2021-nci/","section":"publication","summary":"Given a noun compound (NC), we address the problem of predicting the appropriate semantic label linking the constituents of the NC. This problem is called Noun Compound Interpretation (NCI). We use FrameNet as a semantic label repository. For example, given the noun compound (board approval), we predict the frame (DENY OR GRANT PERMISSION, as per FrameNet) as appropriate and the semantic role of the modifier word (AUTHORITY) as the semantic label linking board and approval; the resulting label is DENY OR GRANT PERMISSION:AUTHORITY. Our semantic label repository is very large (‚âà11k labels) compared to the NC data available for training (approx 1900). Thus, learning in this case, especially for unseen semantic labels, is hard. We propose to solve this problem by predicting semantic labels in a continuous label embedding space, which is novel. This embedding space is created by learning label embeddings using the FrameNet data. The embeddings are then used to train two separate models ‚Äì one for predicting Frames and the other for FEs. As the label embedding space captures the semantics of the labels, using these embeddings enables generalizing well on unseen labels, thus achieving zero-shot learning. Our preliminary investigations show that the proposed approach performs well for unseen labels, achieving 5% and 2% points improvements over baselines for the frame and FE prediction, respectively. The study shows the promise of the use of continuous space embeddings for noun compound interpretation and points to the need for further investigation.","tags":["noun-compound","deep learning","framenet","theoretical"],"title":"FrameNet-assisted Noun Compound Interpretation","type":"publication"},{"authors":["Diptesh Kanojia","Prashant Sharma","Sayali Ghodekar","Pushpak Bhattacharyya","Gholamreza Haffari","Malhar Kulkarni"],"categories":null,"content":"","date":1619136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619136000,"objectID":"9975875309a41d4a552b44995a750bfe","permalink":"https://dipteshkanojia.github.io/publication/eacl-2021-cognate/","publishdate":"2021-04-23T00:00:00Z","relpermalink":"/publication/eacl-2021-cognate/","section":"publication","summary":"Automatic detection of cognates helps downstream NLP tasks of Machine Translation, Cross-lingual Information Retrieval, Computational Phylogenetics and Cross-lingual Named Entity Recognition. Previous approaches for the task of cognate detection use orthographic, phonetic and semantic similarity based features sets. In this paper, we propose a novel method for enriching the feature sets, with cognitive features extracted from human readers' gaze behaviour. We collect gaze behaviour data for a small sample of cognates and show that extracted cognitive features help the task of cognate detection. However, gaze data collection and annotation is a costly task. We use the collected gaze behaviour data to predict cognitive features for a larger sample and show that predicted cognitive features, also, significantly improve the task performance. We report improvements of 10% with the collected gaze features, and 12% using the predicted gaze features, over the previously proposed approaches. Furthermore, we release the collected gaze behaviour data along with our code and cross-lingual models.  *This paper was awarded an honourable mention during the best papers award in the long papers category at EACL 2021.*  ","tags":["best paper honourable mention","gaze tracking","deep learning","cognate detection","experimental","cross lingual"],"title":"Cognition-aware Cognate Detection","type":"publication"},{"authors":["Rudra Murthy","Tamali Banerjee","Jyotsana Khatri","Diptesh Kanojia","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1608285600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608285600,"objectID":"c77a7694ddfde2103b92aaa2174c004f","permalink":"https://dipteshkanojia.github.io/talk/icontutorial-2020/","publishdate":"2020-12-18T10:00:00Z","relpermalink":"/talk/icontutorial-2020/","section":"talk","summary":"The focus of this tutorial is to cover the breadth of the literature on recent advances in Unsupervised Machine Translation. The tutorial will help the audience in getting started with unsupervised machine translation. The tutorial will span over three sections. In the first section, we will cover the fundamental concepts like cross-lingual embeddings, denoising auto-encoders, language model pre-training, Back Translation (BT), etc. which are key to the success of Unsupervised Machine Translation. In the second section, the tutorial will provide a brief summary of recent works on unsupervised machine translation. The tutorial will cover both Phrase-Based Statistical Machine Translation systems as well as Neural Machine Translation systems. In the last section, we will talk about the limitations of the existing approaches for Unsupervised machine translation approaches and provide general guidelines for successful training of these systems. We also discuss case-studies from Indian languages and provide results obtained with U-MT over Indian language pairs. Finally, we talk about possible research directions.","tags":["machine translation","tutorial","unsupervised"],"title":"Unsupervised Neural Machine Translation","type":"talk"},{"authors":["Diptesh Kanojia","Raj Dabre","Shubham Dewangan","Pushpak Bhattacharyya","Gholamreza Haffari","Malhar Kulkarni"],"categories":null,"content":"","date":1606867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"a1858fcf86b499ef0e30b8bad039c554","permalink":"https://dipteshkanojia.github.io/publication/coling-2020-cognate/","publishdate":"2020-12-02T00:00:00Z","relpermalink":"/publication/coling-2020-cognate/","section":"publication","summary":"Cognates are variants of the same lexical form across different languages; for example 'fonema' in Spanish and 'phoneme' in English are cognates, both of which mean 'a unit of sound'. The task of automatic detection of cognates among any two languages can help downstream NLP tasks such as Cross-lingual Information Retrieval, Computational Phylogenetics, and Machine Translation. In this paper, we demonstrate the use of cross-lingual word embeddings for detecting cognates among fourteen Indian Languages. Our approach introduces the use of context from a knowledge graph to generate improved feature representations for cognate detection. We, then, evaluate the impact of our cognate detection mechanism on neural machine translation (NMT), as a downstream task. We evaluate our methods to detect cognates on a challenging dataset of twelve Indian languages, namely, Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu, Punjabi, Bengali, Marathi, and Malayalam. Additionally, we create evaluation datasets for two more Indian languages, Konkani and Nepali. We observe an improvement of up to 18% points, in terms of F-score, for cognate detection. Furthermore, we observe that cognates extracted using our method help improve NMT quality by up to 2.76 BLEU. We also release our code, newly constructed datasets and cross-lingual models publicly.","tags":["cognate detection","machine translation","deep learning","cross-lingual","theoretical"],"title":"Harnessing Cross-lingual Features to Improve Cognate Detection for Low-resource Languages","type":"publication"},{"authors":["Sandeep Mathias","Diptesh Kanojia","Abhijit Mishra","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"e54942e6417a4d853f2e710cd64d19b6","permalink":"https://dipteshkanojia.github.io/publication/ijcai-2020-gazesurvey/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/ijcai-2020-gazesurvey/","section":"publication","summary":"Gaze behaviour has been used as a way to gather cognitive information for a number of years. In this paper, we discuss the use of gaze behaviour in solving different tasks in natural language processing (NLP) without having to record it at test time. This is because the collection of gaze behaviour is a costly task, both in terms of time and money. Hence, in this paper, we focus on research done to alleviate the need for recording gaze behaviour at run time. We also mention different eye tracking corpora in multiple languages, which are currently available and can be used in natural language processing. We conclude our paper by discussing applications in a domain - education - and how learning gaze behaviour can help in solving the tasks of complex word identification and automatic essay grading.","tags":["gaze tracking","deep learning","machine learning","survey","theoretical"],"title":"A Survey on Using Gaze Behaviour for Natural Language Processing","type":"publication"},{"authors":["Sandeep Mathias","Rudra Murthy","Diptesh Kanojia","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"6248fd9598092cf6b443b649790d8e7d","permalink":"https://dipteshkanojia.github.io/publication/icon-2020-aeg/","publishdate":"2020-12-01T12:40:43.631013Z","relpermalink":"/publication/icon-2020-aeg/","section":"publication","summary":"Automatic essay grading (AEG) is a process in which machines assign a grade to an essay written in response to a topic, called the prompt. Zero-shot AEG is when we train a system to grade essays written to a new prompt which was not present in our training data. In this paper, we describe a solution to the problem of zero-shot automatic essay grading, using cognitive information, in the form of gaze behaviour. Our experiments show that using gaze behaviour helps in improving the performance of AEG systems, especially when we provide a new essay written in response to a new prompt for scoring, by an average of almost 5 percentage points of QWK.","tags":["gaze tracking","deep learning","automatic essay grading","experimental","zero-shot"],"title":"Cognitively Aided Zero-Shot Automatic Essay Grading","type":"publication"},{"authors":["Sandeep Mathias","Rudra Murthy","Diptesh Kanojia","Abhijit Mishra","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"7b5f4ef33d9eb67598db163c79e5f98a","permalink":"https://dipteshkanojia.github.io/publication/aacl-2020-aeg/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/publication/aacl-2020-aeg/","section":"publication","summary":"The gaze behaviour of a reader is helpful in solving several NLP tasks such as automatic essay grading. However, collecting gaze behaviour from readers is costly in terms of time and money. In this paper, we propose a way to improve automatic essay grading using gaze behaviour, which is learnt at run time using a multi-task learning framework. To demonstrate the efficacy of this multi-task learning based approach to automatic essay grading, we collect gaze behaviour for 48 essays across 4 essay sets, and learn gaze behaviour for the rest of the essays, numbering over 7000 essays. Using the learnt gaze behaviour, we can achieve a statistically significant improvement in performance over the state-of-the-art system for the essay sets where we have gaze data. We also achieve a statistically significant improvement for 4 other essay sets, numbering about 6000 essays, where we have no gaze behaviour data available. Our approach establishes that learning gaze behaviour improves automatic essay grading.","tags":["gaze tracking","deep learning","automatic essay grading","experimental","multi-tasking"],"title":"Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour","type":"publication"},{"authors":["Diptesh Kanojia"],"categories":null,"content":"","date":1585888200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585888200,"objectID":"116886f40cb508a4020abd7f3c9b3c04","permalink":"https://dipteshkanojia.github.io/talk/csirotalk/","publishdate":"2020-04-03T05:30:00+01:00","relpermalink":"/talk/csirotalk/","section":"talk","summary":"In this talk, I present my primary PhD contributions where different embeddings aid in the tasks of cognate detection, and computational phylogenetics. A set of unpublished results which show that cognates, indeed, help in the downstream task of machine translation were also shown.","tags":["talks","cognate detection","semantics","distributional semantics"],"title":"Investigations into the use of Distributed Semantics for Cognate Detection and Computational Phylogenetics","type":"talk"},{"authors":["Kumar Saurav","Kumar Saunack","Diptesh Kanojia","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"07f94860429b6410e87565e02aa8b937","permalink":"https://dipteshkanojia.github.io/publication/lrec-2020-embeddings/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/lrec-2020-embeddings/","section":"publication","summary":"Dense word vectors or 'word embeddings' which encode semantic properties of words, have now become integral to NLP tasks like Machine Translation (MT), Question Answering (QA), Word Sense Disambiguation (WSD), and Information Retrieval (IR). In this paper, we use various existing approaches to create multiple word embeddings for 14 Indian languages. We place these embeddings for all these languages, viz., Assamese, Bengali, Gujarati, Hindi, Kannada, Konkani, Malayalam, Marathi, Nepali, Odiya, Punjabi, Sanskrit, Tamil, and Telugu in a single repository. Relatively newer approaches that emphasize catering to context (BERT, ELMo, etc.) have shown significant improvements, but require a large amount of resources to generate usable models. We release pre-trained embeddings generated using both contextual and non-contextual approaches. We also use MUSE and XLM to train cross-lingual embeddings for all pairs of the aforementioned languages. To show the efficacy of our embeddings, we evaluate our embedding models on XPOS, UPOS and NER tasks for all these languages. We release a total of 436 models using 8 different approaches. We hope they are useful for the resource-constrained Indian language NLP. The title of this paper refers to the famous novel 'A Passage to India' by E.M. Forster, published initially in 1924.","tags":["word embeddings","cross-lingual word embeddings","indian language embeddings","resource","theoretical"],"title":"\"A Passage to India\": Pre-trained Word Embeddings for Indian Languages","type":"publication"},{"authors":["Diptesh Kanojia","Pushpak Bhattacharyya","Malhar Kulkarni","Gholamreza Haffari"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"307fb61de2abeeed2f98db86e2b11575","permalink":"https://dipteshkanojia.github.io/publication/lrec-2020-cognate/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/lrec-2020-cognate/","section":"publication","summary":"Cognates are present in multiple variants of the same text across different languages (e.g., hund in German and hound in English language mean dog). They pose a challenge to various Natural Language Processing (NLP) applications such as Machine Translation, Cross-lingual Sense Disambiguation, Computational Phylogenetics, and Information Retrieval. A possible solution to address this challenge is to identify cognates across language pairs. In this paper, we describe the creation of two cognate datasets for twelve Indian languages, namely Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu, Punjabi, Bengali, Marathi, and Malayalam. We digitize the cognate data from an Indian language cognate dictionary and utilize linked Indian language Wordnets to generate cognate sets. Additionally, we use the Wordnet data to create a False Friends' dataset for eleven language pairs. We also evaluate the efficacy of our dataset using previously available baseline cognate detection approaches. We also perform a manual evaluation with the help of lexicographers and release the curated gold-standard dataset with this paper.","tags":["cognate detection","false friends detection","cognate dataset","false friends dataset","cross-lingual word embeddings","embeddings","resource","theoretical"],"title":"Challenge Datasets of Cognate and False Friend Pairs for Indian Languages","type":"publication"},{"authors":["Akash Sheoran","Diptesh Kanojia","Aditya Joshi","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"e58703b275c504a68aade802ea2b0b3f","permalink":"https://dipteshkanojia.github.io/publication/lrec-2020-cdsa/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/lrec-2020-cdsa/","section":"publication","summary":"Cross-domain sentiment analysis (CDSA) helps to address the problem of data scarcity in scenarios where labelled data for a domain (known as the target domain) is unavailable or insufficient. However, the decision to choose a domain (known as the source domain) to leverage from is, at best, intuitive. In this paper, we investigate text similarity metrics to facilitate source domain selection for CDSA. We report results on 20 domains (all possible pairs) using 11 similarity metrics. Specifically, we compare CDSA performance with these metrics for different domain-pairs to enable the selection of a suitable source domain, given a target domain. These metrics include two novel metrics for evaluating domain adaptability to help source domain selection of labelled data and utilize word and sentence-based embeddings as metrics for unlabelled data. The goal of our experiments is a recommendation chart that gives the K best source domains for CDSA for a given target domain. We show that the best K source domains returned by our similarity metrics have a precision of over 50%, for varying values of K.","tags":["cross-domain sentiment analysis","sentiment analysis","source domain selection","word embeddings","embeddings","theoretical"],"title":"Recommendation Chart of Domains for Cross-Domain Sentiment Analysis: Findings of A 20 Domain Study","type":"publication"},{"authors":["Diptesh Kanojia","Malhar Kulkarni","Sayali Ghodekar","Eivind Kahrs","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1582156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582156800,"objectID":"81a9a9b67ad295b7ff4b0ae90a0ae543","permalink":"https://dipteshkanojia.github.io/publication/sssu-2020-commentaries/","publishdate":"2020-02-20T00:00:00Z","relpermalink":"/publication/sssu-2020-commentaries/","section":"publication","summary":"This paper describes additional aspects of a digital tool called the ‚ÄòTextual History Tool‚Äô. We describe its various salient features with special reference to those of its features that may help the philologist digitize commentaries and sub-commentaries on a text. This tool captures the historical evolution of a text through various temporal stages, and interrelated data culled from various types of related texts. We use the text of the KƒÅ≈õikƒÅv·πõtti (KV) as a sample text, and with the help of philologists, we digitize the commentaries available to us. We digitize the NyƒÅsa (Ny), the Padama√±jarƒ´ (Pm) and sub commentaries on the KV text known as the Tantrapradƒ´pa (Tp), and the Makaranda (Mk). We divide each commentary and sub-commentary into functional units and describe the methodology and motivation behind the functional unit division. Our functional unit division helps generate more accurate phylogenetic trees for the text, based on distance methods using the data entered in the tool.","tags":["commentaries","textual history","word embeddings","phylogenetics","embeddings","theoretical"],"title":"Strategies of Effective Digitization of Commentaries and Sub-commentaries: Towards the Construction of Textual History","type":"publication"},{"authors":["Diptesh Kanojia","Sravan Munukutla","Sayali Ghodekar","Pushpak Bhattacharyya","Malhar Kulkarni"],"categories":null,"content":"","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"2bae9b13755fd7baf334ac054304a510","permalink":"https://dipteshkanojia.github.io/publication/cods-2020-cognate/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/publication/cods-2020-cognate/","section":"publication","summary":"Automatic Cognate Detection helps NLP tasks of Machine Translation, Information Retrieval, and Phylogenetics. Cognate words are defined as word pairs across languages which exhibit partial or full lexical similarity and mean the same (e.g., hund-hound in German-English). In this paper, we use a Siamese Feed-forward neural network with word-embeddings to detect such word pairs. Our experiments with various embedding dimensions show larger embedding dimensions can only be used for large corpora sizes for this task. On a dataset built using linked Indian Wordnets, our approach beats the baseline approach with a significant margin (up to 71%) with the best F-score of 0.85% on the Hindi-Gujarati language pair.","tags":["word embeddings","cognate detection","indian languages","siamese networks","theoretical"],"title":"\"Keep Your Dimensions on a Leash\": True Cognate Detection using Siamese Deep Neural Networks","type":"publication"},{"authors":["Yashasvi Mantha","Diptesh Kanojia","Pushpak Bhattacharyya","Malhar Kulkarni"],"categories":null,"content":"","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"d26dd4fbece12c6a51f82f1eba233ad4","permalink":"https://dipteshkanojia.github.io/publication/cods-2020-typological/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/publication/cods-2020-typological/","section":"publication","summary":"Establishing language relatedness by inferring phylogenetic trees has been a topic of interest in the area of diachronic linguistics. However, existing methods face meaning conflation deficiency due to the usage of lexical similarity-based measures. In this paper, we utilize fourteen linked Indian Wordnets to create inter-language distances using our novel approach to compute ‚Äòlanguage distances‚Äô. Our pilot study uses deep cross-lingual word embeddings to compute inter-language distances and provide an effective distance matrix to infer phylogenetic trees. We also develop a baseline method using lexical similarity-based metrics for comparison and identify that our approach produces better phylogenetic trees which club related languages closer when compared to the baseline approach.","tags":["word embeddings","phylogenetics","typological trees","cross-lingual word embeddings","theoretical"],"title":"Harnessing Deep Cross-lingual Word Embeddings to Infer Accurate Phylogenetic Trees","type":"publication"},{"authors":["Diptesh Kanojia","Abhijeet Dubey","Malhar Kulkarni","Pushpak Bhattacharyya","Gholamreza Haffari"],"categories":null,"content":"","date":1571961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571961600,"objectID":"bc534447faaf590d1c3fe196bdc48ece","permalink":"https://dipteshkanojia.github.io/publication/iscls-2019-embeddings/","publishdate":"2019-10-25T00:00:00Z","relpermalink":"/publication/iscls-2019-embeddings/","section":"publication","summary":"Tracing the root of a text i.e., the original version of the text, by inferring phylogenetic trees has been a topic of interest in philological studies. However, existing methods face meaning conflation deficiency due to the usage of lexical similarity based measures which feed the distance matrix to clustering algorithms. In this paper, we utilize word embeddings as features to compute the distances among manuscripts. We conduct this pilot study on using word embeddings to compute inter-manuscript distances and provide an effective distance matrix to infer phylogenetic trees. We conduct experiments on the historical Sanskrit text known as KƒÅ≈õikƒÅvrtti (KV) and infer phylogenetic trees using this approach. For comparison, we also develop baseline methods using lexical distance-based measures to infer phylogenetic trees for KV. We show that our methodology produces better trees which club closely related manuscripts together compared to the baseline methods.","tags":["word embeddings","phylogenetics","embeddings","theoretical"],"title":"Utilizing Word Embeddings based Features for Phylogenetic Tree Generation of Sanskrit Texts","type":"publication"},{"authors":["Diptesh Kanojia","Malhar Kulkarni","Pushpak Bhattacharyya","Sayali Ghodekar","Irawati Kulkarni","Nilesh Joshi","Eivind Kahrs"],"categories":null,"content":"","date":1561248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561248000,"objectID":"0a309fb6d49cf4b91cf021d008379445","permalink":"https://dipteshkanojia.github.io/publication/iscls-2019-tht/","publishdate":"2019-06-23T00:00:00Z","relpermalink":"/publication/iscls-2019-tht/","section":"publication","summary":"This paper describes a digital tool called the Textual History Tool in detail. This tool captures the historical evolution of a text through various temporal stages, and inter-related data culled from various types of related texts. This tool also provides a historical view of the transmission of a text through the manuscript tradition. This tool provides an online interface which allows philologists to enter manuscript data for a text. It also provides an online interface which helps philologists compare the variants in a separate mode. It allows the user to generate phylogenetic trees, for the text, based on distance methods using the data entered in the tool. It also contains the facility to generate critical edition using a semi-supervised approach. This tool also divides the text into meaningful functional units and helps achieve a better comparison among the manuscripts. The text of the KV and its textual history is mentioned as a specific example to demostrate the features of this tool.","tags":["embeddings","phylogenetics","tool","online","implementation"],"title":"An Introduction to the Textual History Tool","type":"publication"},{"authors":["Diptesh Kanojia","Kevin Patel","Pushpak Bhattacharyya","Malhar Kulkarni","Gholamreza Haffari"],"categories":null,"content":"","date":1561248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561248000,"objectID":"56e2d449f474e8143cdc46494c93a707","permalink":"https://dipteshkanojia.github.io/publication/gwc-2019-cognate/","publishdate":"2019-06-23T00:00:00Z","relpermalink":"/publication/gwc-2019-cognate/","section":"publication","summary":"Automatic Cognate Detection (ACD) is a challenging task which has been utilized to help NLP applications like Machine Translation, Information Retrieval and Computational Phylogenetics. Unidentified cognate pairs can pose a challenge to these applications and result in a degradation of performance. In this paper, we detect cognate word pairs among ten Indian languages with Hindi and use deep learning methodologies to predict whether a word pair is cognate or not. We identify IndoWordnet as a potential resource to detect cognate word pairs based on orthographic similarity-based methods and train neural network models using the data obtained from it. We identify parallel corpora as another potential resource and perform the same experiments for them. We also validate the contribution of Wordnets through further experimentation and report improved performance of up to 26%. We discuss the nuances of cognate detection among closely related Indian languages and release the lists of detected cognates as a dataset. We also observe the behaviour of, to an extent, unrelated Indian language pairs and release the lists of detected cognates among them as well.","tags":["cognate detection","phylogenetics","wordnets","theoretical"],"title":"Utilizing Wordnets for Cognate Detection among Indian Languages","type":"publication"},{"authors":["Diptesh Kanojia","Malhar Kulkarni","Pushpak Bhattacharyya","Gholamreza Haffari"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"61c3322c291fdf21d26a7391097536ee","permalink":"https://dipteshkanojia.github.io/publication/kanojia-2019-cognate/","publishdate":"2019-06-27T12:40:43.659033Z","relpermalink":"/publication/kanojia-2019-cognate/","section":"publication","summary":"Cognates are present in multiple variants of the same text across different languages. Computational Phylogenetics uses algorithms and techniques to analyze these variants and infer phylogenetic trees for a hypothesized accurate representation based on the output of the computational algorithm used. In our work, we detect cognates among a few Indian languages namely Hindi, Marathi, Punjabi, and Sanskrit for helping build cognate sets for phylogenetic inference. Cognate detection helps phylogenetic inference by helping isolate diachronic sound changes and thus detect the words of a common origin. A cognate set manually annotated with the help of a lexicographer is generally used to automatically infer phylogenetic trees. Our work creates cognate sets of each language pair and infers phylogenetic trees based on a bayesian framework using the Maximum likelihood method. We also implement our work to an online interface and infer phylogenetic trees based on automatically detected cognate sets. The online interface helps create phylogenetic trees based on the textual data provided as an input. It helps a lexicographer provide manual input of data, edit the data based on their expert opinion and eventually create phylogenetic trees based on various algorithms including our work on automatically creating cognate sets. We go on to discuss the nuances in detection cognates with respect to these Indian languages and also discuss the categorization of Cognate words i.e., 'Tatasama' and 'Tadbhava' words.","tags":["cognate detection","wordnets","linguistics","phylogenetics"],"title":"Cognate Identification to improve Phylogenetic trees for Indian Languages","type":"publication"},{"authors":["Swaraja Salaskar","Diptesh Kanojia","Malhar Kulkarni"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c8545d3558190c1f5e7c488362e5cbf7","permalink":"https://dipteshkanojia.github.io/publication/salaskar-2019-some/","publishdate":"2019-06-27T12:40:43.659403Z","relpermalink":"/publication/salaskar-2019-some/","section":"publication","summary":"In today‚Äôs digital world language technology has gained importance. Several software, have been developed and are available in the field of computational linguistics. Such tools play a crucial role in making classical language texts easily accessible. Some Indian philosophical schools have contributed towards various techniques of verbal cognition to analyze sentence correctly. These theories can be used to build computational tools for word sense disambiguation (WSD). In the absence of WSD, one cannot have proper verbal cognition. These theories considered the concept of ‚ÄòYogyatƒÅ‚Äô (congruity or compatibility) as the indispensable cause of verbal cognition. In this work, we come up with some insights on the basis of these theories to create a tool that will capture YogyatƒÅ of words. We describe the problem of ambiguity in a text and present a method to resolve it computationally with the help of YogyatƒÅ. Here, only two major schools i.e. NyƒÅya and VyƒÅkara·πáa are considered. Our paper attempts to show the implication of the creation of our tool in this area. Also, our tool involves the creation of an ‚Äòontological tag-set‚Äô as well as strategies to mark up the lexicon. The introductory description of ablation is also covered in this paper. Such strategies and some case studies shall form the core of our paper.","tags":["word sense disambiguation","sanskrit","linguistics"],"title":"Some Strategies to Capture Karaka-Yogyata with Special Reference to apadana","type":"publication"},{"authors":["Diptesh Kanojia"],"categories":null,"content":"","date":1544878800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544878800,"objectID":"c46e5d205d31b278be6c8ed9867f9473","permalink":"https://dipteshkanojia.github.io/talk/techfestalk/","publishdate":"2018-12-15T13:00:00Z","relpermalink":"/talk/techfestalk/","section":"talk","summary":"In this talk, I discuss Natural Language Processing, Data Science and how they are related to each other. While presenting the different facets of the NLP research done at our lab, I connected it to how a data scientist can use the research to solve a relatable real-world issue. I discussed the NLP research areas of Machine Translation, Sentiment Analysis, Sarcasm Detection, Speech Processing, Cognitive NLP and then went into details of Computational Phylogenetics and Cognate Detection, the latter two being a part of my Ph.D. Thesis. I also presented some excerpts of his work from a recent publication at CODS-COMAD 2019 and explained how Cognate detection is an important problem in the area of Historical and Cultural Linguistics. The talk was well received as the feedback from the students and some entrepreneurs were positive. Later, it transcended more into an interactive session on what tools can an amateur use to start NLP and what online courses can one pursue to deal with these NLP problems.","tags":["talks"],"title":"Natural Language Processing and its intersection with Data Science","type":"talk"},{"authors":["Sandeep Mathias","Diptesh Kanojia","Kevin Patel","Samarth Agarwal","Abhijit Mishra","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"b0de83230d96792faac13f9cd22589dc","permalink":"https://dipteshkanojia.github.io/publication/mathias-2018-eyes/","publishdate":"2019-06-27T12:40:43.658566Z","relpermalink":"/publication/mathias-2018-eyes/","section":"publication","summary":"Predicting a reader's rating of text quality is a challenging task that involves estimating different subjective aspects of the text, like structure, clarity, etc. Such subjective aspects are better handled using cognitive information. One such source of cognitive information is gaze behaviour. In this paper, we show that gaze behaviour does indeed help in effectively predicting the rating of text quality. To do this, we first we model text quality as a function of three properties - organization, coherence and cohesion. Then, we demonstrate how capturing gaze behaviour helps in predicting each of these properties, and hence the overall quality, by reporting improvements obtained by adding gaze features to traditional textual features for score prediction. We also hypothesize that if a reader has fully understood the text, the corresponding gaze behaviour would give a better indication of the assigned rating, as opposed to partial understanding. Our experiments validate this hypothesis by showing greater agreement between the given rating and the predicted rating when the reader has a full understanding of the text.","tags":["essay grading","gaze tracking"],"title":"Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour","type":"publication"},{"authors":["Hanumant Redkar","Rajita Shukla","Sandhya Singh","Jaya Saraswati","Laxmi Kashyap","Diptesh Kanojia","Preethi Jyothi","Malhar Kulkarni","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"6c2143e1be7cf1a37b9ff8d925206eb7","permalink":"https://dipteshkanojia.github.io/publication/redkar-2018-hindi/","publishdate":"2019-06-27T12:40:43.636641Z","relpermalink":"/publication/redkar-2018-hindi/","section":"publication","summary":"This paper reports the work related to making Hindi Wordnet1 available as a digital resource for language learning and teaching, and the experiences and lessons that were learnt during the process. The language data of the Hindi Wordnet has been suitably modified and enhanced to make it into a language learning aid. This aid is based on modern pedagogical axioms and is aligned to the learning objectives of the syllabi of the school education in India. To make it into a comprehensive language tool, grammatical information has also been encoded, as far as these can be marked on the lexical items. The delivery of information is multi-layered, multi-sensory and is available across multiple digital platforms. The front end has been designed to offer an eye-catching user-friendly interface which is suitable for learners starting from age six onward. Preliminary testing of the tool has been done and it has been modified as per the feedbacks that were received. Above all, the entire exercise has offered gainful insights into learning based on associative networks and how knowledge based on such networks can be made available to modern learners.","tags":["teaching","hindi","wordnets","linguistics"],"title":"Hindi Wordnet for Language Teaching: Experiences and Lessons Learnt","type":"publication"},{"authors":["Diptesh Kanojia","Kevin Patel","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"9224743bc44e4b49ccb53dfd30ef6bc7","permalink":"https://dipteshkanojia.github.io/publication/kanojia-2018-indian/","publishdate":"2019-06-27T12:40:43.637105Z","relpermalink":"/publication/kanojia-2018-indian/","section":"publication","summary":"Wordnets are rich lexico-semantic resources. Linked wordnets are extensions of wordnets, which link similar concepts in wordnets of different languages. Such resources are extremely useful in many Natural Language Processing (NLP) applications, primarily those based on knowledge-based approaches. In such approaches, these resources are considered as gold standard/oracle. Thus, it is crucial that these resources hold correct information. Thereby, they are created by human experts. However, human experts in multiple languages are hard to come by. Thus, the community would benefit from sharing of such manually created resources. In this paper, we release mappings of 18 Indian language wordnets linked with Princeton WordNet. We believe that availability of such resources will have a direct impact on the progress in NLP for these languages.","tags":["api","wordnets","mapping"],"title":"Indian Language Wordnets and their Linkages with Princeton WordNet","type":"publication"},{"authors":["Jayashree Gajjam","Diptesh Kanojia","Malhar Kulkarni"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"c234725d46b01454d96696ff671e0c0f","permalink":"https://dipteshkanojia.github.io/publication/gajjam-2018-new/","publishdate":"2019-06-27T12:40:43.658152Z","relpermalink":"/publication/gajjam-2018-new/","section":"publication","summary":"A sentence is an important notion in the Indian grammatical tradition. The collection of the definitions of a sentence can be found in the text ‚ÄòVƒÅkyapadƒ´ya‚Äô written by Bhart·πõhari in fifth century C.E. The grammarian-philosopher Bhart·πõhari and his authoritative work ‚ÄòVƒÅkyapadƒ´ya‚Äô have been a matter of study for modern scholars, at least for more than 50 years, since Ashok Aklujkar submitted his Ph.D. dissertation at Harvard University. The notions of a sentence and a word as a meaningful linguistic unit in the language have been a subject matter for the discussion in many works that followed later on. While some scholars have applied philological techniques to critically establish the text of the works of Bhart·πõhari, some others have devoted themselves to exploring philosophical insights from them. Some others have studied his works from the point of view of modern linguistics, and psychology. Few others have tried to justify the views by logical discussions. In this paper, we present a fresh view to study Bhart·πõhari, and his works, especially the ‚ÄòVƒÅkyapadƒ´ya‚Äô. This view is from the field of Natural Language Processing (NLP), more specifically, what is called as Cognitive NLP. We have studied the definitions of a sentence given by Bhart·πõhari at the beginning of the second chapter of ‚ÄòVƒÅkyapadƒ´ya‚Äô. We have researched one of these definitions by conducting an experiment and following the methodology of silent-reading of Sanskrit paragraphs. We collect the Gaze-behavior data of participants and analyze it to understand the underlying comprehension procedure in the human mind and present our results. We evaluate the statistical significance of our results using T-test, and discuss the caveats of our work. We also present some general remarks on this experiment and usefulness of this method for gaining more insights in the work of Bhart·πõhari.","tags":["gaze tracking","sanskrit","linguistics","cognitive"],"title":"New Vistas to study Bhart·πõhari: Cognitive NLP","type":"publication"},{"authors":["Ritesh Panjwani","Diptesh Kanojia","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"4a96343e39d82e05e1e94b8c95451501","permalink":"https://dipteshkanojia.github.io/publication/panjwani-2018-pyiwn/","publishdate":"2019-06-27T12:40:43.636412Z","relpermalink":"/publication/panjwani-2018-pyiwn/","section":"publication","summary":"Indian language WordNets have their individual web-based browsing interfaces along with a common interface for IndoWordNet. These interfaces prove to be useful for language learners and in an educational domain, however, they do not provide the functionality of connecting to them and browsing their data through a lucid application programming interface or an API. In this paper, we present our work on creating such an easy-to-use framework which is bundled with the data for Indian language WordNets and provides NLTK WordNet interface like core functionalities in Python. Additionally, we use a pre-built speech synthesis system for Hindi language and augment Hindi data with audios for words, glosses, and example sentences. We provide a detailed usage of our API and explain the functions for ease of the user. Also, we package the IndoWordNet data along with the source code and provide it openly for the purpose of research. We aim to provide all our work as an open source framework for further development.","tags":["api","wordnets","application"],"title":"pyiwn: A Python-based API to access Indian Language WordNets","type":"publication"},{"authors":["Kevin Patel","Diptesh Kanojia","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"e549b5253c882cd4600363d7eed5ab7a","permalink":"https://dipteshkanojia.github.io/publication/patel-2018-semi/","publishdate":"2019-06-27T12:40:43.636185Z","relpermalink":"/publication/patel-2018-semi/","section":"publication","summary":"Wordnets are rich lexico-semantic resources. Linked wordnets are extensions of wordnets, which link similar concepts in wordnets of different languages. Such resources are extremely useful in many Natural Language Processing (NLP) applications, primarily those based on knowledge-based approaches. In such approaches, these resources are considered as gold standard/oracle. Thus, it is crucial that these resources hold correct information. Thereby, they are created by human experts. However, manual maintenance of such resources is a tedious and costly affair. Thus techniques that can aid the experts are desirable. In this paper, we propose an approach to link wordnets. Given a synset of the source language, the approach returns a ranked list of potential candidate synsets in the target language from which the human expert can choose the correct one(s). Our technique is able to retrieve a winner synset in the top 10 ranked list for 60% of all synsets and 70% of noun synsets.","tags":["mapping","wordnets","linguistics"],"title":"Semi-automatic WordNet Linking using Word Embeddings","type":"publication"},{"authors":["Diptesh Kanojia","Preethi Jyothi","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"956d431caad5bf8b1c040719082e4143","permalink":"https://dipteshkanojia.github.io/publication/kanojia-2018-synthesizing/","publishdate":"2019-06-27T12:40:43.635953Z","relpermalink":"/publication/kanojia-2018-synthesizing/","section":"publication","summary":"In this paper, we describe our work on the creation of a voice model using a speech synthesis system for the Hindi Language. We use pre-existing 'voices', use publicly available speech corpora to create a 'voice' using the Festival Speech Synthesis System (Black, 1997). Our contribution is two-fold: (1) We scrutinize multiple speech synthesis systems and provide an extensive report on the currently available state-of-the-art systems. We also develop voices using the existing implementations of the aforementioned systems, and (2) We use these voices to generate sample audios for randomly chosen words; manually evaluate the audio generated, and produce audio for all WordNet words using the winner voice model. We also produce audios for the Hindi WordNet Glosses and Example sentences. We describe our efforts to use pre-existing implementations for WaveNet - a model to generate raw audio using neural nets (Oord et al., 2016) and generate speech for Hindi. Our lexicographers perform a manual evaluation of the audio generated using multiple voices. A qualitative and quantitative analysis reveals that the voice model generated by us performs the best with an accuracy of 0.44.","tags":["speech synthesis","wordnets","linguistics","application"],"title":"Synthesizing Audio for Hindi Wordnet","type":"publication"},{"authors":["Diptesh Kanojia","Nikhil Wani","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"1d266404a0de73edef83e42e47f05123","permalink":"https://dipteshkanojia.github.io/publication/kanojia-2017-your/","publishdate":"2019-06-27T12:40:43.636877Z","relpermalink":"/publication/kanojia-2017-your/","section":"publication","summary":"We present a quantitative, data-driven machine learning approach to mitigate the problem of unpredictability of Computer Science Graduate School Admissions. In this paper, we discuss the possibility of a system which may help prospective applicants evaluate their Statement of Purpose (SOP) based on our system output. We, then, identify feature sets which can be used to train a predictive model. We train a model over fifty manually verified SOPs for which it uses an SVM classifier and achieves the highest accuracy of 92% with 10-fold cross validation. We also perform experiments to establish that Word Embedding based features and Document Similarity based features outperform other identified feature combinations. We plan to deploy our application as a web service and release it as a FOSS service","tags":["application","sop","prediction"],"title":"Is your Statement Purposeless? Predicting Computer Science Graduation Admission Acceptance based on Statement Of Purpose","type":"publication"},{"authors":["Aditya Joshi","Diptesh Kanojia","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"84a032536fc3c0c35ab1a828c11cb8ef","permalink":"https://dipteshkanojia.github.io/publication/joshi-2017-sarcasm/","publishdate":"2019-06-27T12:40:43.635477Z","relpermalink":"/publication/joshi-2017-sarcasm/","section":"publication","summary":"Sarcasm Suite is a browser-based engine that deploys Ô¨Åve of our past papers in sarcasm detection and generation. The sarcasm detection modules use four kinds of incongruity: sentiment incongruity, semantic incongruity, historical context incongruity and conversational context incongruity. The sarcasm generation module is a chatbot that responds sarcastically to user input. With a visually appealing interface that indicates predictions using ‚Äòfaces‚Äô of our co-authors from our past papers, Sarcasm Suite is our Ô¨Årst demonstration of our work in computational sarcasm.","tags":["sarcasm detection","application","linguistics"],"title":"Sarcasm Suite: A browser-based engine for sarcasm detection and generation","type":"publication"},{"authors":["Abhijit Mishra","Diptesh Kanojia","Seema Nagar","Kuntal Dey","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"b348baa075331214499696a99528fcf3","permalink":"https://dipteshkanojia.github.io/publication/mishra-2017-scanpath/","publishdate":"2019-06-27T12:40:43.635707Z","relpermalink":"/publication/mishra-2017-scanpath/","section":"publication","summary":"Measuring reading effort is useful for practical purposes such as designing learning material and personalizing text comprehension environment. We propose a quantification of reading effort by measuring the complexity of eye-movement patterns of readers. We call the measure Scanpath Complexity. Scanpath complexity is modeled as a function of various properties of gaze fixations and saccades- the basic parameters of eye movement behavior. We demonstrate the effectiveness of our scanpath complexity measure by showing that its correlation with different measures of lexical and syntactic complexity as well as standard readability metrics is better than popular baseline measures based on fixation alone.","tags":["gaze tracking","theoretical"],"title":"Scanpath Complexity: Modeling Reading Effort using Gaze Information","type":"publication"},{"authors":["Diptesh Kanojia"],"categories":null,"content":"","date":1466409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466409600,"objectID":"84d17c1c55af735d59da07b46f39b679","permalink":"https://dipteshkanojia.github.io/talk/vivatalk/","publishdate":"2016-06-20T09:00:00+01:00","relpermalink":"/talk/vivatalk/","section":"talk","summary":"In three different talks, I discussed various facets of fundamental NLP research being done at CFILT, IIT Bombay. The first talk one was about 'Lexical Resources' and their need in NLP tasks. During the second talk, I discussed the basics of POS tagging, Viterbi and fundamentals of NLP layers. The third talk was more of a demo session where I conducted an interactive hands-on session using Python and NLTK to demonstrate the basics.","tags":[],"title":"NLP Fundamentals at VIVA IET","type":"talk"},{"authors":null,"categories":null,"content":"One of my first few projects in Android development. I created and hosted a webview based Android application.\n","date":1461711600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461711600,"objectID":"6997b6545d3d8c63fbd89798a80f1f4f","permalink":"https://dipteshkanojia.github.io/project/brahminet/","publishdate":"2016-04-27T00:00:00+01:00","relpermalink":"/project/brahminet/","section":"project","summary":"A webview based app for BrahmiNet.","tags":["android"],"title":"BrahmiNet - Android","type":"project"},{"authors":null,"categories":null,"content":"One of my first few projects in Android development. I created and hosted a webview based Android application.\n","date":1461711600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461711600,"objectID":"57eb5cf8a3d0474d5a7df35b12ffc540","permalink":"https://dipteshkanojia.github.io/project/hindiwordnet/","publishdate":"2016-04-27T00:00:00+01:00","relpermalink":"/project/hindiwordnet/","section":"project","summary":"A webview based app for Hindi Wordnet.","tags":["android"],"title":"Hindi Wordnet - Android","type":"project"},{"authors":null,"categories":null,"content":"","date":1461711600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461711600,"objectID":"ae2088b800f7c0cca01611b1a57885e6","permalink":"https://dipteshkanojia.github.io/project/hwnchromex/","publishdate":"2016-04-27T00:00:00+01:00","relpermalink":"/project/hwnchromex/","section":"project","summary":"Browser Extension for Google Chrome","tags":["extension"],"title":"HWN Chrome Browser Extension","type":"project"},{"authors":null,"categories":null,"content":"One of my first few projects in Android development. I created and hosted a webview based Android application.\n","date":1461711600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461711600,"objectID":"30c6baeab52eaf3e6a0dfe9218e04f1c","permalink":"https://dipteshkanojia.github.io/project/marathiwordnet/","publishdate":"2016-04-27T00:00:00+01:00","relpermalink":"/project/marathiwordnet/","section":"project","summary":"A webview based app for Hindi Wordnet.","tags":["android"],"title":"Marathi Wordnet - Android","type":"project"},{"authors":null,"categories":null,"content":"","date":1461711600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461711600,"objectID":"4593405f22c9d5ca7ae6b27bb12def6e","permalink":"https://dipteshkanojia.github.io/project/mwnchromex/","publishdate":"2016-04-27T00:00:00+01:00","relpermalink":"/project/mwnchromex/","section":"project","summary":"Browser Extension for Google Chrome","tags":["extension"],"title":"MWN Chrome Browser Extension","type":"project"},{"authors":null,"categories":null,"content":"One of my first few projects in Android development. I created and hosted a webview based Android application.\n","date":1461711600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461711600,"objectID":"c95fdefec7d6521563a933fd0837f5d1","permalink":"https://dipteshkanojia.github.io/project/sanskritwordnet/","publishdate":"2016-04-27T00:00:00+01:00","relpermalink":"/project/sanskritwordnet/","section":"project","summary":"A webview based app for Hindi Wordnet.","tags":["android"],"title":"Sanskrit Wordnet - Android","type":"project"},{"authors":null,"categories":null,"content":"","date":1461711600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461711600,"objectID":"a076694eafe98efb8d2238d3a32e545a","permalink":"https://dipteshkanojia.github.io/project/swnchromex/","publishdate":"2016-04-27T00:00:00+01:00","relpermalink":"/project/swnchromex/","section":"project","summary":"Browser Extension for Google Chrome","tags":["extension"],"title":"SWN Chrome Browser Extension","type":"project"},{"authors":["Diptesh Kanojia","Shehzaad Dhuliawala","Pushpak Bhattarcharyya"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"007bd1b41fccf006ff9b3e1b795aafa9","permalink":"https://dipteshkanojia.github.io/publication/kanojia-2016-picture/","publishdate":"2019-06-27T12:40:43.633369Z","relpermalink":"/publication/kanojia-2016-picture/","section":"publication","summary":"WordNet has proved to be immensely useful for Word Sense Disambiguation, and thence Machine translation, Information Retrieval and Question Answering. It can also be used as a dictionary for educational purposes. The semantic nature of concepts in a WordNet motivates one to try to express this meaning in a more visual way. In this paper, we describe our work of enriching IndoWordNet with image acquisitions from the OpenClipArt library. We describe an approach used to enrich WordNets for eighteen Indian languages. Our contribution is three fold: (1) We develop a system, which, given a synset in English, finds an appropriate image for the synset. The system uses the OpenclipArt library (OCAL) to retrieve images and ranks them. (2) After retrieving the images, we map the results along with the linkages between Princeton WordNet and Hindi WordNet, to link several synsets to corresponding images. We choose and sort top three images based on our ranking heuristic per synset. (3) We develop a tool that allows a lexicographer to manually evaluate these images. The top images are shown to a lexicographer by the evaluation tool for the task of choosing the best image representation. The lexicographer also selects the number of relevant images. Using our system, we obtain an Average Precision (P @ 3) score of 0.30.","tags":["images","wordnets","mapping","application"],"title":"A picture is worth a thousand words: Using OpenClipArt library for enriching IndoWordNet","type":"publication"},{"authors":["Diptesh Kanojia","Vishwajeet Kumar","Krithi Ramamritham"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"f4d2b17c0c321e9b803e773952062fcb","permalink":"https://dipteshkanojia.github.io/publication/kanojia-2016-civique/","publishdate":"2019-06-27T12:40:43.635248Z","relpermalink":"/publication/kanojia-2016-civique/","section":"publication","summary":"We present the Civique system for emergency detection in urban areas by monitoring micro blogs like Tweets. The system detects emergency related events, and classifies them into appropriate categories like 'fire', 'accident', 'earthquake', etc. We demonstrate our ideas by classifying Twitter posts in real time, visualizing the ongoing event on a map interface and alerting users with options to contact relevant authorities, both online and offline. We evaluate our classifiers for both the steps, i.e., emergency detection and categorization, and obtain F-scores exceeding 70% and 90%, respectively. We demonstrate Civique using a web interface and on an Android application, in realtime, and show its use for both tweet detection and visualization.","tags":["social media","emergency detection","application"],"title":"Civique: Using Social Media to detect Urban Emergencies","type":"publication"},{"authors":["Abhijit Mishra","Diptesh Kanojia","Seema Nagar","Kuntal Dey","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"6baf9e15971aa9affc5e05ded84ab3c0","permalink":"https://dipteshkanojia.github.io/publication/mishra-2016-harnessing/","publishdate":"2019-06-27T12:40:43.634769Z","relpermalink":"/publication/mishra-2016-harnessing/","section":"publication","summary":"In this paper, we propose a novel mechanism for enriching the feature vector, for the task of sarcasm detection, with cognitive features extracted from eye-movement patterns of human readers. Sarcasm detection has been a challenging research problem, and its importance for NLP applications such as review summarization, dialog systems and sentiment analysis is well recognized. Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds. This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text. We observe the difference in the behaviour of the eye, while reading sarcastic and non sarcastic sentences. Motivated by this observation, we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data. We perform statistical classification using the enhanced feature set so obtained. The augmented cognitive features improve sarcasm detection by 3.7% (in terms of F-score), over the performance of the best reported system","tags":["sarcasm detection","gaze tracking","empirical"],"title":"Harnessing Cognitive Features for Sarcasm Detection","type":"publication"},{"authors":["Abhijit Mishra","Diptesh Kanojia","Seema Nagar","Kuntal Dey","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"3558e0b7f7eed704ca1626519923f10b","permalink":"https://dipteshkanojia.github.io/publication/mishra-2016-leveraging/","publishdate":"2019-06-27T12:40:43.635012Z","relpermalink":"/publication/mishra-2016-leveraging/","section":"publication","summary":"Sentiments expressed in user-generated short text and sentences are nuanced by subtleties at lexical, syntactic, semantic and pragmatic levels. To address this, we propose to augment traditional features used for sentiment analysis and sarcasm detection, with cognitive features derived from the eye-movement patterns of readers. Statistical classification using our enhanced feature set improves the performance (F-score) of polarity detection by a maximum of 3.7% and 9.3% on two datasets, over the systems that use only traditional features. We perform feature significance analysis, and experiment on a held-out dataset, showing that cognitive features indeed empower sentiment analyzers to handle complex constructs","tags":["sentiment analysis","gaze tracking"],"title":"Leveraging Cognitive Features for Sentiment Analysis","type":"publication"},{"authors":["Meghna Singh","Rajita Shukla","Jaya Jha","Laxmi Kashyap","Diptesh Kanojia","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"2712c8503089fc117cb14c2987c656b5","permalink":"https://dipteshkanojia.github.io/publication/singh-2016-mapping/","publishdate":"2019-06-27T12:40:43.633831Z","relpermalink":"/publication/singh-2016-mapping/","section":"publication","summary":"This paper reports the work of creating bilingual mappings in English for certain synsets of Hindi wordnet, the need for doing this, the methods adopted and the tools created for the task. Hindi wordnet, which forms the foundation for other Indian language wordnets, has been linked to the English WordNet. To maximize linkages, an important strategy of using direct and hypernymy linkages has been followed. However, the hypernymy linkages were found to be inadequate in certain cases and posed a challenge due to sense granularity of language. Thus, the idea of creating bilingual mappings was adopted as a solution. A bilingual mapping means a linkage between a concept in two different languages, with the help of translation and/or transliteration. Such mappings retain meaningful representations, while capturing semantic similarity at the same time. This has also proven to be a great enhancement of Hindi wordnet and can be a crucial resource for multilingual applications in natural language processing, including machine translation and cross language information retrieval.","tags":["bilingual","mapping","wordnets","linguistics"],"title":"Mapping it differently: A solution to the linking challenges","type":"publication"},{"authors":["Abhijit Mishra","Diptesh Kanojia","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"73d9745ec7166ccb72abc4147eb724e3","permalink":"https://dipteshkanojia.github.io/publication/mishra-2016-predicting/","publishdate":"2019-06-27T12:40:43.634067Z","relpermalink":"/publication/mishra-2016-predicting/","section":"publication","summary":"Sarcasm understandability or the ability to understand textual sarcasm depends upon readers‚Äô language proficiency, social knowledge, mental state and attentiveness. We introduce a novel method to predict the sarcasm understandability of a reader. Presence of incongruity in textual sarcasm often elicits distinctive eye-movement behavior by human readers. By recording and analyzing the eye-gaze data, we show that eye-movement patterns vary when sarcasm is understood vis-√†-vis when it is not. Motivated by our observations, we propose a system for sarcasm understandability prediction using supervised machine learning. Our system relies on readers‚Äô eye-movement parameters and a few textual features, thence, is able to predict sarcasm understandability with an F-score of 93%, which demonstrates its efficacy. The availability of inexpensive embedded-eye-trackers on mobile devices creates avenues for applying such research which benefits web-content creators, review writers and social media analysts alike.","tags":["sarcasm detection","understandability","gaze tracking","sentiment analysis"],"title":"Predicting Readers' Sarcasm Understandability by Modeling Gaze Behavior","type":"publication"},{"authors":["Shehzaad Dhuliawala","Diptesh Kanojia","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"980d4342984c1727c712290357a16e2f","permalink":"https://dipteshkanojia.github.io/publication/dhuliawala-2016-slangnet/","publishdate":"2019-06-27T12:40:43.634303Z","relpermalink":"/publication/dhuliawala-2016-slangnet/","section":"publication","summary":"We present a WordNet like structured resource for slang words and neologisms on the internet. The dynamism of language is often an indication that current language technology tools trained on today's data, may not be able to process the language in the future. Our resource could be (1) used to augment the WordNet, (2) used in several Natural Language Processing (NLP) applications which make use of noisy data on the internet like Information Retrieval and Web Mining. Such a resource can also be used to distinguish slang word senses from conventional word senses. To stimulate similar innovations widely in the NLP community, we test the efficacy of our resource for detecting slang using standard bag of words Word Sense Disambiguation (WSD) algorithms (Lesk and Extended Lesk) for English data on the internet.","tags":["slang","wordnets","word sense disambiguation","theoretical"],"title":"SlangNet: A WordNet like resource for English Slang","type":"publication"},{"authors":["Diptesh Kanojia","Raj Dabre","Pushpak Bhattarcharyya"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"ee20b0bc8136ae310c91029a752bc0f4","permalink":"https://dipteshkanojia.github.io/publication/kanojia-2016-sophisticated/","publishdate":"2019-06-27T12:40:43.633603Z","relpermalink":"/publication/kanojia-2016-sophisticated/","section":"publication","summary":"India is a country with 22 officially recognized languages and 17 of these have WordNets, a crucial resource. Web browser based interfaces are available for these WordNets, but are not suited for mobile devices which deters people from effectively using this resource. We present our initial work on developing mobile applications and browser extensions to access WordNets for Indian Languages. Our contribution is two fold: (1) We develop mobile applications for the Android, iOS and Windows Phone OS platforms for Hindi, Marathi and Sanskrit WordNets which allow users to search for words and obtain more information along with their translations in English and other Indian languages. (2) We also develop browser extensions for English, Hindi, Marathi, and Sanskrit WordNets, for both Mozilla Firefox, and Google Chrome. We believe that such applications can be quite helpful in a classroom scenario, where students would be able to access the WordNets as dictionaries as well as lexical knowledge bases. This can help in overcoming the language barrier along with furthering language understanding.","tags":["application","wordnets","linguistics"],"title":"Sophisticated Lexical Databases - Simplified Usage: Mobile Applications and Browser Plugins For Wordnets","type":"publication"},{"authors":["Diptesh Kanojia","Aaditya Joshi","Pushpak Bhattacharyya","Mark J. Carman"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"831f4ce63e9dc687d409cbee4f75f9e4","permalink":"https://dipteshkanojia.github.io/publication/kanojia-2016-ll/","publishdate":"2019-06-27T12:40:43.63453Z","relpermalink":"/publication/kanojia-2016-ll/","section":"publication","summary":"Parallel corpora are often injected with bilingual lexical resources for improved Indian language machine translation (MT). In absence of such lexical resources, multilingual topic models have been used to create coarse lexical resources in the past, using a Cartesian product approach. Our results show that for morphologically rich languages like Hindi, the Cartesian product approach is detrimental for MT. We then present a novel 'sentential' approach to use this coarse lexical resource from a multilingual topic model. Our coarse lexical resource when injected with a parallel corpus outperforms a system trained using parallel corpus and a good quality lexical resource. As demonstrated by the quality of our coarse lexical resource and its benefit to MT, we believe that our sentential approach to create such a resource will help MT for resource-constrained languages.","tags":["topic models","machine translation","application"],"title":"That‚Äôll do fine!: A coarse lexical resource for English-Hindi MT, using polylingual topic models","type":"publication"},{"authors":["Diptesh Kanojia","Shehzaad Dhuliawala","Naman Gupta","Abhijit Mishra","Pushpak Bhattarcharyya"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"ac46b0af2c34eb3074ad66030d8ff02a","permalink":"https://dipteshkanojia.github.io/publication/kanojia-2015-transchat/","publishdate":"2019-06-27T12:40:43.633143Z","relpermalink":"/publication/kanojia-2015-transchat/","section":"publication","summary":"We present TransChat, an open-source, cross platform, Indian language Instant Messaging (IM) application that facilitates cross lingual textual communication over English and multiple Indian Languages. The application is a client-server IM architecture based chat system with multiple Statistical Machine Translation (SMT) engines working towards efficient translation and transmission of messages. TransChat allows users to select their preferred language and internally, selects appropriate translation engine based on the input configuration. For translation quality enhancement, necessary pre- and post-processing steps are applied on the input and output chat texts. We demonstrate the efficacy of TransChat through a series of qualitative evaluations that test- (a) The usability of the system (b) The quality of the translation output. In a multilingual country like India, such applications can help overcome language barrier in domains like tourism, agriculture and health.","tags":["machine translation","chat","application"],"title":"TransChat: Cross-Lingual Instant Messaging for Indian Languages","type":"publication"},{"authors":["Diptesh Kanojia","Aaditya Joshi","Pushpak Bhattarcharyya","Mark J. Carman"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"2b9f28d206c6f9573b1e1e46062646d5","permalink":"https://dipteshkanojia.github.io/publication/kanojia-2015-using/","publishdate":"2019-06-27T12:40:43.632923Z","relpermalink":"/publication/kanojia-2015-using/","section":"publication","summary":"Parallel corpora are often injected with bilingual dictionaries for improved Indian language machine translation (MT). In absence of such dictionaries, a coarse dictionary may be required. This paper demonstrates the use of a multilingual topic model for creating coarse dictionaries for English-Hindi MT. We compare our approaches with: (a) a baseline with no additional dictionary injection, and (b) a corpus with a good quality dictionary. Our results show that the existing Cartesian product approach which is used to create the pseudo-parallel data results in a degradation on tourism and health datasets, for English-Hindi MT. Our paper points to the fact that existing Cartesian approach using multilingual topics (devised for European languages) may be detrimental for Indian language MT. On the other hand, we present an alternate ‚Äòsentential‚Äô approach that leads to a slight improvement. However, our sentential approach (using a parallel corpus injected with a coarse dictionary) outperforms a system trained using parallel corpus and a good quality dictionary.","tags":["topic models","machine translation","application"],"title":"Using Multilingual Topic Models for Improved Alignment in English-Hindi MT","type":"publication"},{"authors":["Hanumant Harichandra Redkar","Sudha Baban Bhingardive","Diptesh Kanojia","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"bd887cc7ad0ca678dfd99c5c00ddf912","permalink":"https://dipteshkanojia.github.io/publication/redkar-2015-world/","publishdate":"2019-06-27T12:40:43.632029Z","relpermalink":"/publication/redkar-2015-world/","section":"publication","summary":"WordNet is an online lexical resource which expresses unique concepts in a language. English WordNet is the first WordNet which was developed at Princeton University. Over a period of time, many language WordNets were developed by various organizations all over the world. It has always been a challenge to store the WordNet data. Some WordNets are stored using file system and some WordNets are stored using different database models. In this paper, we present the World WordNet Database Structure which can be used to efficiently store the WordNet information of all languages of the World. This design can be adapted by most language WordNets to store information such as synset data, semantic and lexical relations, ontology details, language specific features, linguistic information, etc. An attempt is made to develop Application Programming Interfaces to manipulate the data from these databases. This database structure can help in various Natural Language Processing applications like Multilingual Information Retrieval, Word Sense Disambiguation, Machine Translation, etc.","tags":["wordnets","demo","application"],"title":"World WordNet database structure: an efficient schema for storing information of WordNets of the world","type":"publication"},{"authors":["Diptesh Kanojia","Pushpak Bhattacharyya","Raj Dabre","Siddhartha Gunti","Manish Shrivastava"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"8fb4d28ee4dfcc35196262f68fd2225a","permalink":"https://dipteshkanojia.github.io/publication/kanojia-2014-not/","publishdate":"2019-06-27T12:40:43.631806Z","relpermalink":"/publication/kanojia-2014-not/","section":"publication","summary":"The task of Word Sense Disambiguation (WSD) incorporates in its definition the role of ‚Äòcontext‚Äô. We present our work on the development of a tool which allows for automatic acquisition and ranking of ‚Äòcontext clues‚Äô for WSD. These clue words are extracted from the contexts of words appearing in a large monolingual corpus. These mined collection of contextual clues form a discrimination net in the sense that for targeted WSD, navigation of the net leads to the correct sense of a word given its context. Utilizing this resource we intend to develop efficient and light weight WSD based on look up and navigation of memory-resident knowledge base, thereby avoiding heavy computation which often prevents incorporation of any serious WSD in MT and search. The need for large quantities of sense marked data too can be reduced.","tags":["wordnets","word sense disambiguation","theoretical"],"title":"Do not do processing, when you can look up: Towards a Discrimination Net for WSD","type":"publication"},{"authors":["Diptesh Kanojia","Manish Shrivastava","Raj Dabre","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"a5bb7954e38e4049139a6644ccfab71d","permalink":"https://dipteshkanojia.github.io/publication/kanojia-2014-pacman/","publishdate":"2019-06-27T12:40:43.63225Z","relpermalink":"/publication/kanojia-2014-pacman/","section":"publication","summary":"We present a Parallel Corpora Management tool that aides parallel corpora generation for the task of Machine Translation (MT). It takes source and target text of a corpus for any language pair in text file format, or zip archives containing multiple corresponding text files. Then, it provides with a helpful interface to lexicographers for manual translation / validation, and gives out the corrected text files as output. It provides various dictionary references as help within the interface which increase the productivity and efficiency of a lexicographer. It also provides automatic translation of the source sentence using an integrated MT system. The tool interface includes a corpora management system which facilitates maintenance of parallel corpora by assigning roles such as manager, lexicographer etc. We have designed a novel tool that provides aides like references to various dictionary sources such as Wordnets, Shabdkosh, Wikitionary etc. We also provide manual word alignment correction which is visualized in the tool and can lead to its gamification in the future, thus, providing a valuable source of word / phrase alignments.","tags":["machine translation","corpus management","application"],"title":"PaCMan: Parallel Corpus Management Workbench","type":"publication"},{"authors":["Neha R Prabhugaonkar","Apurva S Nagvenkar","Diptesh Kanojia","Jyoti D. Pawar","Pushpak Bhattacharyya","Manish Shrivastava"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"14afd32d7d582d2abf7b2f5a21261a3c","permalink":"https://dipteshkanojia.github.io/publication/prabhugaonkar-2014-panchbhoota/","publishdate":"2019-06-27T12:40:43.632478Z","relpermalink":"/publication/prabhugaonkar-2014-panchbhoota/","section":"publication","summary":"We present our work on developing fifteen Hierarchical Phrase Based Statistical Machine Translation (HPBSMT) systems for five Indian language pairs namely Bengali-Hindi, English-Hindi, Marathi-Hindi, Tamil-Hindi, and Telugu-Hindi, in three domains each, HEALTH, TOURISM and GENERAL. We named them PanchBhoota, as these systems are elemental in nature. We used a very simple approach to train, tune, and test them using 'cdec' toolkit. We hope that this work will motivate Indian Language Machine Translation researchers to look deeper into the field of HPBSMT which is known to perform better than Phrase Based Statistical Machine Translation","tags":["winner","machine translation","linguistics","shared task","competition"],"title":"PanchBhoota: Hierarchical phrase based machine translation systems for five Indian languages","type":"publication"},{"authors":["Salil Joshi","Diptesh Kanojia","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"d666c7337ff369f3e55a383f15eeedda","permalink":"https://dipteshkanojia.github.io/publication/joshi-2013-more/","publishdate":"2019-06-27T12:40:43.631473Z","relpermalink":"/publication/joshi-2013-more/","section":"publication","summary":"Word Sense Disambiguation (WSD) approaches have reported good accuracies in recent years. However, these approaches can be classified as weak AI systems. According to the classical definition, a strong AI based WSD system should perform the task of sense disambiguation in the same manner and with similar accuracy as human beings. In order to accomplish this, a detailed understanding of the human techniques employed for sense disambiguation is necessary. Instead of building yet another WSD system that uses contextual evidence for sense disambiguation, as has been done before, we have taken a step back - we have endeavored to discover the cognitive faculties that lie at the very core of the human sense disambiguation technique. In this paper, we present a hypothesis regarding the cognitive sub-processes involved in the task of WSD. We support our hypothesis using the experiments conducted through the means of an eye-tracking device. We also strive to find the levels of difficulties in annotating various classes of words, with senses. We believe, once such an in-depth analysis is performed, numerous insights can be gained to develop a robust WSD system that conforms to the principle of strong AI.","tags":["gaze tracking","word sense disambiguation","theoretical"],"title":"More than meets the eye: Study of Human Cognition in Sense Annotation","type":"publication"},{"authors":["Arindam Chatterjee","Salil Joshi","Pushpak Bhattacharyya","Diptesh Kanojia","Akhlesh Kumar Meena"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"cb5f13494aed36490a8f152e07b74d5d","permalink":"https://dipteshkanojia.github.io/publication/chatterjee-2012-study/","publishdate":"2019-06-27T12:40:43.631013Z","relpermalink":"/publication/chatterjee-2012-study/","section":"publication","summary":"Does context help determine sense? This question might seem frivolous, even preposterous to anybody sensible. However, our long time research on Word Sense Disambiguation (WSD) shows that in almost all disambiguation algorithms, the sense distribution parameter P(S/W), where P is the probability of the sense of a word W being S, plays the deciding role. The widely reported accuracy figure of around 60% for all-words-domain-independent WSD is contributed to mainly by P(S/W), as one ablation test after another reveals. The story with human annotation is different though. Our experience of working with human annotators who mark with WordNet sense ids, general and domain specific corpora brings to light the interesting fact that producing sense ids without looking at the context is a heavy cognitive load. Sense annotators do form hypothesis in their minds about the possible sense of a word (‚Äòmost frequent sense‚Äô bias), but then look at the context for clues to accept or reject the hypothesis. Such clues are minimal, just one or two words, but are critical nonetheless. Without these clues the annotator is left in an indecisive state as to whether or not to put down the first sense coming to his mind. The task becomes all the more cognitively challenging, if the senses are fine grained and seem equally probable. These facts increase the annotation time by a factor of almost 1.5. In the current paper we explore the dichotomy that might exist between machines and humans in the way they determine senses. We study the various parameters for WSD and also the sense marking behavior of human sense annotators. The observations, though not completely conclusive, establish the need for context for humans and that for accurate sense distribution parameters for machines.","tags":["word sense disambiguation","theoretical"],"title":"A Study of the Sense Annotation Process: Man v/s Machine.","type":"publication"},{"authors":["Diptesh Kanojia","Arindam Chatterjee","Salil Joshi","Pushpak Bhattacharyya"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"90073a90cba2d3992fee7ae2988160f3","permalink":"https://dipteshkanojia.github.io/publication/kanojia-2012-discrimination/","publishdate":"2019-06-27T12:40:43.632706Z","relpermalink":"/publication/kanojia-2012-discrimination/","section":"publication","summary":"Current state-of-the-art Word Sense Disambiguation (WSD) algorithms are mostly supervised and use the P (Sense|Word) statistic for annotation. This P (Sense|Word) statistic is obtained after training the model on an annotated corpus. The performance of WSD algorithms do not match the efficiency and quality of human annotation. It is therefore important to know the role of the contextual clues in WSD. Human beings in turn, actuate the task of disambiguating the sense of a word, by gathering hints from the context words in the neighbourhood of the word. Contextual clues thus form the basic building block for the human sense disambiguation task. The need was thus felt for a tool, which could help us get a deeper insight into the human mind, while disambiguating polysemous words. As mentioned earlier, in the human mind, sense disambiguation highly depends on finding clues in corpus text, which finally lead to a winner sense. In order to make WSD algorithms more efficient, it is highly desirable to assimilate knowledge regarding contextual clues of words. In order to make WSD algorithms more efficient, it is highly desirable to assimilate knowledge regarding contextual clues of words, which aid in finding correct senses of words in that context. Hence, we developed a tool which could help a lexicographer mark the clues for disambiguating a word in a context. In the current phase, this tool lets the lexicographer select the clues from the gloss and example fields in the synset, and adds them to a database.","tags":["word sense disambiguation","applciation","linguistics"],"title":"Discrimination-net for Hindi","type":"publication"}]