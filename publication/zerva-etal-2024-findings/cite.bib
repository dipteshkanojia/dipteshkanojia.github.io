@inproceedings{zerva-etal-2024-findings,
    title = "Findings of the Quality Estimation Shared Task at {WMT} 2024: Are {LLM}s Closing the Gap in {QE}?",
    author = "Zerva, Chrysoula  and
      Blain, Frederic  and
      C. De Souza, Jos{\'e} G.  and
      Kanojia, Diptesh  and
      Deoghare, Sourabh  and
      Guerreiro, Nuno M.  and
      Attanasio, Giuseppe  and
      Rei, Ricardo  and
      Orasan, Constantin  and
      Negri, Matteo  and
      Turchi, Marco  and
      Chatterjee, Rajen  and
      Bhattacharyya, Pushpak  and
      Freitag, Markus  and
      Martins, Andr{\'e}",
    editor = "Haddow, Barry  and
      Kocmi, Tom  and
      Koehn, Philipp  and
      Monz, Christof",
    booktitle = "Proceedings of the Ninth Conference on Machine Translation",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.wmt-1.3/",
    doi = "10.18653/v1/2024.wmt-1.3",
    pages = "82--109",
    abstract = "We report the results of the WMT 2024 shared task on Quality Estimation, in which the challenge is to predict the quality of the output of neural machine translation systems at the word and sentence levels, without access to reference translations. In this edition, we expanded our scope to assess the potential for quality estimates to help in the correction of translated outputs, hence including an automated post-editing (APE) direction. We publish new test sets with human annotations that target two directions: providing new Multidimensional Quality Metrics (MQM) annotations for three multi-domain language pairs (English to German, Spanish and Hindi) and extending the annotations on Indic languages providing direct assessments and post edits for translation from English into Hindi, Gujarati, Tamil and Telugu. We also perform a detailed analysis of the behaviour of different models with respect to different phenomena including gender bias, idiomatic language, and numerical and entity perturbations. We received submissions based both on traditional, encoder-based approaches as well as large language model (LLM) based ones."
}